All terms and points are covered here in a way I understood it.

https://linkmeup.gitbook.io/sdsm/0.-planirovanie

#                    Network Design
Tier 2 network - Access and Distribution layer
Tier 3 network - Access, Distribution and Core layer
Collapsed topology - can be collapsed in following ways:
    Core-Distribution (2 modules), Distribution-Access (2 modules), Core-Distribution-Access (1 module)

L2 Access layer:
    STP have to be considered - links will be blocked (solution - PVST+ or assigning certain VLANs to certain links with no loops)
L3 Access layer:
    Costs more money but STP and Loops are not an issue anymore. Also can't easily stretch the VLAN

Modern techs to get rid of STP:
    Chassis switches (prevents loops, and no single point of failure)
    Stacking (prevents loops, and no single point of failure)

Access layer - devices, L2 switches
    Port security - MACs, 802.1x;
    QoS;
    VLAN tagging.
Distribution layer - SVIs, East-West traffic, sometimes L3 interfaces
    SVIs;
    FHRPs;
    ACLs;
    Routing protocols.
Core layer - high-speed, high-convergence, high-throughput, access to Internet
    RPs;
    fast l3 switching

DC new cool topology:
    Spine-Leaf

#                    High availability (HA)
FHRP (VRRP, HSRP, GLBP):
VRRP - virtual gateway protocol. 2+ gateways. Can't load balance. Common on all devices.
    "Master" and "Backup" routers
    VRRP exchanges hello messages over the timers (defaults are 3sec for HELLO, 10sec for HOLD)
    Can use one of physical router IPs as virtual. In that case, the router with that IP is an Active by default.
    VRRP authentication is recommended (MD5)
    VRRP priority (1-254). 100 is default. The bigger the better.
    After Master router switchover, "preemt" is required to be switched back over to previous active one. Preemt feature is *enabled by default
    VRRPv2 and VRRPv3 (used for IPv6) exist.
    VMAC = 0000.5e00.01** (256 different ones)
    MulticastIP = 224.0.0.18
    VRRP on Nexus with VPC (in DC) acts same as HSRP

HSRP - Cisco proprietary VRRP implementation. 2+ gateways
    "Active" and "Standby" routers. "Listen" as well
    HSRP exchanges hello messages over the timers (defaults are 3sec for HELLO, 10sec for HOLD)
    HSRP authentication is recommended (MD5)
    HSRP priority (0-255). 100 is default. The bigger the better.
    Preempt feature is *disabled by default
    Each HSRP process requires an HSRP group
    HSRPv1: 
        256 HSRP groups; 
        VMAC = 0000.0c07.ac** (256 different ones);
        MulticastIP = 224.0.0.2
    HSRPv2:
        4096 HSRP groups;
        VMAC = 0000.0c9f.f*** (4096 different ones);
        MulticastIP = 224.0.0.102;
        Millisecond timers are configurable.
    HSRP on Nexus with VPC acts differently. Usually all the traffic goes to Active switch and in VPC scenario it'll have to go through standby sometimes. That's why in given scenario, Standby switch handles the packets just like active. But it's still standby if checked hsrp status on switches. But some requests (like ARP) will still go to Active and be responded from it.
    It's a good idea to LB using HSRP by making one router primary for one VLAN and other router primary for other VLAN.
    
After switches get assigned a virtual IP, also they receive a virtual MAC-address. After first ARP from client, primary switch responces to packet. Whenever it's down and there appears a new primary switch which takes over the virtual IP (virtual MAC as well), it sends a Gratuitous ARP to say "now i have that MAC"
GARP - An ARP response when there were to ARP request.
FHRP object tracking helps to change the VIP holder to another device in case of some event. Following can be tracked:
    Route availability;
    Line protocol of an interface;
    Prefix reachability;
    IP SLA.

GLBP - Load balancing mechanism (Cisco proprietary). Similar to HSRP, but supports no less than 4 gateways
    "Active Virtual Gateway" (AVG) and "Active Virtual Forwarder" (AVF) states of router
    Traffic from connected endpoints is being sent to one of AVFs (AVG is also an AVF) and by that, gets load-balanced
    AVG is choosed based on priority
    Can be load-balanced based on:
        Round-robin
        Weighted
        Host-dependent (based on which host is sending the traffic)
    Timers and authentication are present as well
    VMAC = 0007.B40x.xxyy (x.xx is a Group ID, limited by 1024 different ones, since first "x" is 4 bits; yy is an AVF identifier which is "01-04", can be checkes by show glbp commands)
    MulticastIP = 224.0.0.102 (like HSRPv2)
    
SSO - mechanism that allows second CPU to take over the traffic and processing from other CPU in case of it's failure
    Can be used if device has two Route processes (control planes) and one is going down
NSF - Nonstop Forwarding. Allows data plane to send out packets to a neighbor even if it goes down control plane - wise.
    NSF feature must be configured on both devices to be operable
    Routes are being marked as Stale and will be exchanged once again when CP (control plane) goes up
VSS - stacking mechanism that allows two physical switches to be represented as a single virtual switch. Devices can be located in remote locations


#                    WLAN (wireless LAN) design
Comparison of basic Access devices (switches) to Autonomous access points:
    SVI (switched virtual interface)   ---    BVI (bridged virtual interface)
    VLAN                               ---    SSID

Autonomous design (old)
    Main issue for Autonomous deployment is low scalability. For Core deployments connected through L3, VLAN cannot be stretched reliably for clients to be able to move from one physical place to another and still be connected to same VLAN.
    Also it's difficult to configure a lot of device manually.
    Previously used to help configure a lot of devices. But there still were issues with pushing the config.
    WLSM (Wireless LAN Services Module) - EOS 
    WLSE (Wireless LAN Solution Engine) - EOS

Lightweight design (modern)
    WLC (Wireless LAN Controller)
    Access points are brainless and are used in conjunction with the controller (WLC). Once they're connected to network, they receive an IP through DHCP, retrieve Sotware image and configuration.
    Clients are connected to APs. APs establish a data tunnel (CAPwAP - Control and Provisioning of Wireless Access Points) so clients are connected directly to WLC from L2 perspective. Both Data plane and Control plane traffic are going through the tunnel.

WLC deployment design models
    Centralized (APs are in campus, WLC is in DC, tunnel is used to exchange traffic between APs and WLC).
        Pros: L2 Roam - VLANs are located on all APs, users can safely move from anywhere to anywhere and still be on their same subnet.
        Cons: Logically clients are located in the DC which is not quite secure. Also licensing should be considered. WLC controllers can manage only certain number of devices
    Distributed (Same as Centralized, but there are different WLCs for every building).
        Pros: Still L2 Roam within the building
        Cons: Licensing (1 WLC for certain number of APs) and number of controllers is increased.
    Branch (Flex connect) - used for small buildings. APs and WLC are deployed as in Centralised model but tunnel established between them only transfers Control Plane traffic. Data plane remains at AP and is sent out directly to Switch.
        Requirements: <100ms latency to WLC, <50 APs
        Pros: Efficient traffic route (no need to send traffic to DC first). Simpler WLC topology.
        Cons: No more L2 roam, but since it's a branch design it might not be needed.
    Cloud (Meraki) - Wireless controllers are located in the cloud. APs are installed on site and they transfer control plane traffic to the cloud. DP is going to upstream swtiches.
        Requirements: Cisco Meraki APs
        Pros: No licensing issues (increased with the number of used APs), no need to manage controller Hardware. Data traffic of wireless clients is passed to local switch instead of being tunneled to controller
        Cons: No L2 roam in case the Distribution switches (or access switches) are L3 interconnected 

Wireless AP deployment and placement
    COVERAGE
        Coverage is about wireless signal being available at every part of the office/campus/building with (preferrably) the same good strength. It can overlap between different APs and it actually is a standard. 35% overlap (for voice and video networks) and 20% overlap (for no video/voice) is desirable.
        When checking the signal of certain AP, end of coverage should be considered at the point where strength gets changed from -67 dBm to -68 dBm.
        Wireless CHANNELS should also be considered. There are 2 common ones: 2.4 GHz and 5 GHz.
        APs can have different antennas. Ones that give a signal to 360 degrees and ones that direct the signal in some angle (10-60 degrees)
        Cisco models can be with internal antennas (AP-I) or external (AP-E). External models antennas can be changed.
    CAPACITY
        Capacity is about number of clients per AP. 
        When preparing Wireless design, it should be considered how many people will use the AP to understand how many APs you need 

Cisco Prime Infrastructure - network management software platform. It provides a centralized management solution. The software allows administrators to monitor, configure, and troubleshoot network devices from a single interface, and can also be used for network performance analysis and reporting.

Real Time Location Services (RTLS) - service that helps to understand where does the certain device is physically located in the building (it have to be using WIFI at the moment)
    Can be used with RFID tags to check the location of anything (wheelchair in hospital for example)
    Also if it leaves certain area, there can be an alert
    
    Data is collected by WLC and APs but a separate Software is required to gather and analyze the data
        CPI
        CMX
        DNA Center (SDA)
        DNA Spaces (SDA)

    Accuracy. 
        Basic: We can always tell which AP is device connected to.
        Advanced: Triangulation. Assume there are 3 APs. A wireless device is in a reach of every AP (ex. -42 dBm, -59 dBm, -63 dBm). Every dBm level can be drawn as a circle around each AP. When 3 circles are drawn, there's just a single point where all of them interconnect. Like that location can be determined. But only in a free space without any walls.
                  ***(have to clarify, too tired to write it down correctly) RF Fingerprinting. Technic of profiling the wireless coverage. Just walk around with wireless device around the block for APs to understand how it all look like logically.


#                    Cloud computing solutions
Identification of Cloud.
According to NIST (National Institute of Standards and Technology), some system can be identified as a Cloud computing solution (example is Public Cloud like AWS, Microsoft Azure, Google Cloud Platform) if:
    1. It supports self-service - Customer can log in to account and just spin up a new VM or service.
    2. It has a broad network access - It is highly available from as much regions of the world as possible.
    3. It supports resource pooling - Anyones VM in cloud can be running with few other VMs from other companies. Multitenancy is used for companies to be secured from each other.
    4.* It supports Rapid Elasticity - Resources for clients VMs can be smartly (by itself) expanded whenever the demand is higher than usual and shrinked back to normal when demand falls off.
    5.* Services are Metered - All the usage of resources is being counted just like electricity in houses. Customers pay for all their usage monthly (usually monthly but there can be exceptions).

Cloud models
    No cloud:
        In no cloud environment (basic DataCenter), your systems engineers are responsible for spinning up a VM, installing an OS on it, and an Application usually.
        It can look like below:
        _______    _______    _______
        |  ___ |   |  ___ |   |  __  |
        | |APP||   | |WEB||   | |DB| |
        |  ‾‾‾ |   |  ‾‾‾ |   |  ‾‾  |
        |  __  |   |  __  |   |  __  |
        | |OS| |   | |OS| |   | |OS| |
        |  ‾‾  |   |  ‾‾  |   |  ‾‾  |
        |___VM1|   |___VM2|   |___VM3|
       
        3 VMs, one with OS and Application, another with OS and WEB-server, third one is with OS and DB. All deployed and supported by your company
        (VM, OS, APP, WEB, DB)
    
    IAAS (Infrastructure as a service). Cost $
        VMs are deployed by Cloud provider. You only control which OS is being installed and what services are there
        (OS, APP, WEB, DB)
    
    PAAS (Platform as a service). Cost $$
        VMs and OS are deployed by Cloud provider. You only control applications, WEB-server and DB
        (APP, WEB, DB)
    
    SAAS (Software as a service). Cost $$$
        VMs, OS, WEB-server, DB are deployed by Cloud provider. You only control your Application and support it.
        (APP)
    
Cloud Deployment Models
    Public - all the companies resources are on the cloud. Highly available, but data is stored on remote servers and VMs are on same hardware with other companies.    
    Private - all the companies resources are on the cloud resources dedicated specifically for this company. 
    Hybrid - a combination of both Public and Private cloud. Some apps might be published to public, some to hybrid, it can depend on the sensitivity of stored data.
    Community - a cloud environment created by two or more organizations for access for all of parties to a single common cloud of services

Pros of certain models:
    Private: 
        Security (you can restrict access to the resources);
        Data sovereignity (you own the data therefore it's yours only);
        Lower cost when a lot of users.
    Public:
        Elasticity (you can grow the amount of resources whenever needed or shrink them);
        Services (you can get some services that can't be obtained in Private model, like serverless computing);
        Simplicity (just run servers and use).
    Hybrid:
        All pros of above models are included

In Hybrid model, Cloud brokering is involved.
Users (when spinning up an application) are going to cloud broker, Like Cisco Cloud Center. And then they can choose where to deploy their APP:
    Private cloud;
    Public cloud (AWS);
    Public cloud (Azure);
    Public cloud (GCP).

#                    SD-WAN
SDN (Software-Defined Network) - network that is managed in a centralized way (though a controller or using scripts)
Additionally SDN have an underlay and an overlay. And usually tunnels is used to make sure overlay works as intended
SD-WAN  is an SDN applied to devices related to WAN (edge router for example)

SD-WAN  is separated into 4 planes (Orchestration, Management, Control and Data)
In usual network, Control and Data plane are both living on same device.
In case of SD-WAN (and SDN), controller is responsible for Control Plane (brain of network). Edge devices are responsible for Data plane (legs of network).

Differences:
WAN                                                             |   SD-WAN
________________________________________________________________|__________________________________________________________________________
Each device has own control plane                               |   Centralized control plane
All configuration, upgrades and monitoring done on each box     |   Configuration, upgrades and monitoring done centralized
Automation only through scripts                                 |   Automation can be done using API or scripts
New installation done from scratch                              |   New installation can be done with help of ZTP (Zero-Touch Provisioning)

Cisco solution was initially iWAN (intelligent WAN) but that failed and they made a purchase of Viptela company.
Currect solution is "Cisco SD-WAN". Also they have a Meraki SD-WAN solution

Cisco SD-WAN components:
    vManage - User logs in to it and configures all the settings required to be run on devices. Centralized management and monitoring happens here. Can be configured using REST API.
        DEMO: https://sandbox-sdwan-2.cisco.com/ (devnetuser/RG!_Yw919_83)
        Templates are used to push the predefined configuration to devices.
    vSmart - A controller, CAN ONLY BE VIRTUAL (public/private cloud or on-prem). It sends out the configuration (that has been configured on vManage) to routers. Config like Security/QoS/VPNs
    vEdge - Basically a Router. It is interconnected with other routers via IPsec tunnel. So underlay is a Provider network. Overlay is tunnels
        vEdge can be deployed in a Cloud therefore be virtual.
        Also there's a cEdge - Cisco ISRs with Viptela code running on them. Used to be able to support T1, E1 (Voice, serial) lines, or some security features
        Represented by Cisco devices (ASR 1k, ISR 1k, ISR 4k, ENCS 5400), device runs IOS-XE 18.3.0+
        It is not recommended to run both vEdges and cEdges in the same location
    vBond - An orchestrator (virtual deployment only, it must have a public IP) with OOB view on the SD-WAN. Used to glue all the components together. Orchestrates the connectivity between vEdges and vSmart controller.
        Whenever new vEdge device join SD-WAN, it goes to vBond which sends out the info to vManage. Then Administrator can approve or reject adding the vEdge to SD-WAN
        Can help for two vEdges to build an IPsec tunnel if both of them are behind NAT - that is called NAT traversal
        Example: it sends out a packet to two vEdges which open the certain ports. Devices then use that ports to build a tunnel. Or it can orchestrate so those devices exchange packets themselves.
                
Cisco SD-WAN topologies:
    Full mesh (requires license)
    Partial mesh (requires license)
    Point-to-Point (kind of requires license)
    Hub-and-Spoke (no license required)
    
    There are 4 different options in which SD-WAN solution can work whenever there are more than 1 WAN circuit (WAN or INTERNET):
        Active-Active (traffic is fairly splitted between two circuits)
        Active-Active weighted (traffic is splitted between two links but amount sent through each depends on the bandwidth) 
        Active-Standby (traffic is sent through Active only and is being failovered in case of issues on Active)
        Application-aware SLA - used to be able to see circuit degradation even if it's still up. Used to track the availability of some metrics for the circuit. DPI (required additional license) can help to identify the application.

SD-WAN Programmability
    NETCONF - a protocol that allows to send a configuration (XML) to devices using SSH
    REST APIs - a protocol that allows HTTP/HTTPS requests (GET, POST, DELETE, PUT) to be sent to some device. That allows to receive status and configuration of system

vManage GUI is actually a set of REST API replies received by sending the requests through the web page.
vSmart receives configuration changes from vManage by NETCONF
vEdges receive configuration from vSmart through OMP (Overlay Management Protocol)

Controller deployment models
    Public model: all components are deployed (and MANAGED!) by Cisco in their AWS space (2x vSmarts active/active; 2x vBonds Active/Active; 2x vManage Active/Standby). All of them have to be reachable through public internet space
    Hybrid model: controllers are now deployed in company's public cloud, or in the DC. VMs are Managed by the companies engineers. Their public IPs have to be reachable through all WAN circuits.
    Hybrid model w/private IPs: same as hybrid models but private IPs are used. In that case NATting is required since vBond have to have a PUBLIC IP-address.

Cisco SD-WAN hardware
    vEdge-100b (100Mbit connectivity max, both download&upload, ethernet only)
    vEdge-100m (100 but supports 2G/3G/4G modem)
    vEdge-100wm (100 but mobile + wireless)
    vEdge-1000 (1Gig connectivity throughput)
    vEdge-2000 (10Gig connectivity throughput)
    vEdge-5000 (20Gig connectivity throughput)


#                    SD-Access
SDN applied to Access layer of network.

Traditional challenges:
    STP
    Manual config
    Poor scalability
    Difficult security policy

SD-Access is Campus Fabric with DNA center as a controller for the network.
Campus Fabric is:
    VXLAN used to build tunnels as overlay mechanism
    **(LISP) - Location Identifier Separation Protocol
    CTS (ISE basically) - Cisco TrustSec
    
Cisco SD-Access layers:
    Physical (equipment)
        Control plane node (LISP) - a device responsible for storing the data of mappings (mappings DB) of each device in the network to other device
        Fabric border device - Core
        Fabric intermediate device - Distribution
        Fabric edge device - Access
        Fabric WLC
    Network (logic and physics of the network)
        VXLAN - Data plane. Builds direct tunnels but don't have information of where to direct traffic to.
            additional 50 Bytes are used for the encapsulation. MTU of underlay should be increased by that number
        LISP - Control plane. Directs the traffic to required switch or node of network.
        CTS (cisco trustsec) - Policy plane. Used to enforce existing policies to ongoing traffic.
    Controller (a controller of whole solution + ISE)
        Cisco DNA center
            Network control platform
                APIC-EM
                used for Automation (underlay & overlay), CLI/SNMP, NETCONF/YANG
            Network data platform
                Assurance -> Collect data
                SNMP, Netflow, Telemetry        
    Management (GUI, REST APIs, CLI)
        4-step workflow
            1. Design (conf settings, profiles, definitions)
            2. Policy (set policy (ISE, AAA and stuff))
            3. Provision (assign role to a device)
            4. Assurance (monitoring of health of network)

SDN implementation and effect upon planes:
Imperative approach - Only controller has control plane. In case devices loose connectivity to controller - they're dead since no decision can be made
Declarative approach - Devices has their own control plane which replicate the state from controller. In case connectivity to controller is lost, devices can handle traffic on their own




#                    QoS (Quality of service)
QoS - allows the manipulation of traffic (packets) in a way to prioritize some important data to be transfered with better confidions (sometimes be transferred at all - in case bandwidth of an interface is lower that the amount of traffic)

QoS tools:
1. Classification & Marking - used for ingress traffic. Packets get classified (by source, destination IP, protocol, ports) and marked on ingress interface with certain priority for egress interface to understand how it should be forwarded. 
    NBAR (Network-Based Application Recognition) exists which lets the classification to happen with more details (based on word, phrase or URL for example)
2. Policing - used on ingress interface (provide edge interface facing customer edge device) to drop all the traffic that exceeds the configured policing bandwidth. Can be configured for different types of traffic (TCP, UDP, DNS, other apps.)
3. Shaping - used on egress interface to create queues (queueing) which allows traffic that exceeds the configured bandwidth to be sent later, whenever the circuit is less busy. 
4. Queueing - used on egress interface to divide egress queue on sub-queues
5. Scheduling - used to empty the sub-queues in a certain way.

Congestion management - a set of tools (egress interface) that manages happened congestion. WFQ, CBWFQ, PQ, LLQ, WRR, SRR, Shaping.
Congestion avoidance - a set of tools (ingress interface) that prevents congestions from happening. RED, WRED, WTD, Policing.

QoS application in Network:
Integrated services - unified settings on all the routers on the way. Uses RSVP (Resource Reservation Protocol).
Differentiated services - each router has it's own settings. Uses PHB (Per-Hop Behaviour).

QoS Policies
Before all QoS settings had to be applied to interfaces.
Now QoS settings are applied globally. Interfaces are used to display the direction of traffic for QoS to be applied.

MQS (Modular QoS Command-Line):
QoS applied globally.
Allows for different QoS tools to be applied to different interfaces.
3 components:
  Class-map - used to match certain type of traffic and classify it
  Policy-map - used to match certain traffic classified by class-maps. To initiate the use of class-maps
  Service-policy - used to apply a policy-map to interface. Inbound/Outbound.
  
1. Traffic from 10.0.0.1 to 10.255.1.1 is matched and classified by class-map.
2. Policy-map initiates the class-map and makes the classified traffic to follow the specified rules. Apply policing for all the traffic from 10.0.0.1 to 10.255.1.1 (from gi0/1 to gi0/3) to prevent it from congesting the interface gi0/1.
3. Apply service-policy to interface gi0/1 (inbound) and interface gi0/3 (outbout) to use the above rules for this traffic.

#                    Network device architecture
Process Switching - old intra-device frame/packet switching mechanism. Every frame/packet received by device is forwarded to CPU for it to lookup the MAC / Routing table and decide where Data unit should be egressed.

RIB - a "Database" of all the information about learned routes on the device. Routing Table includes a formatted data taken from RIB.
CEF (Cisco Express Forwarding) - Cisco proprietary mechanism for DataUnit switching. After packet was once forwarded through CPU, same source/destination combination will not be forwarded to CPU anymore and will just be egresses to the same interface as before. Data for that is stored in FIB (network - network mask - egress interface)
In recent CEF implementations, all the FIB data is collected beforehand from RIB.
CEF establishes an area (Cached area) that stores pre-defined decisions for packet switching.
CEF consists of FIB and Adjacency table.

FIB - database of all the routes in a compact format (network - network mask - egress interface)

Adjacency table - an L2 information with MAC-addresses for packet to be reincapsulated into a new frame (with new source and destination MACs) while forwarding.

https://linkmeup.ru/podcasts/959/ --- CEF, Process Switching deep dive

CAM - located in a RAM. Contains the information about MAC addresses of devices in subnets. MACs are represented as MAC-addrrss table. Can only be used by switches
TCAM - located in RAM. Contains IP-addresses and subnet masks. Provides as assistance with decision of best path based on longest match (/30 mask is longer match than /24). Subnets and masks are represented as Routing table. Can be used by routers and multilayer switches.
RP (route processor)

#                    Virtualization
#        ENDPOINT VIRTUALIZATION 
Hypervisor - a utility that allows management and provides visibility of Guest OSes (VMs)
Type 1 hypervisor - a hypervisor that is installed directly on Hardware (HW --- Hypervisor --- VMs)
Type 2 hypervisor - a hypervisor that is installed on some OS like Microsoft Windows (HW --- OS --- Hypervisor --- VMS)

Virtual Switch (vSwitch) - literally a virtual switch  that connects it's virtual interfaces (ports) to VNICs distributed to running VMs. 
vSwitch can distiribute certain connected devices to their own broadcast domains ("VLANs"), this function is represented by port-groups.
Virtual Switches exist on type1 Hypervisors by default (ESXi vSwitch, Microsoft Hyper-V)

#        DATA PATH VIRTUALIZATION
VRF - A technology that allows router (or Multilayer device) to logically be divided into few other routers.
Each VRF will have their own routing table (for example that allows few similar subnets to be defined and reachable.
Usually used in ISP networks therefore requires ISP networks (MPLS, VPN, L3VPN, BGP).

VRF-Lite - an Enterprise solution that allows routing to be done through common routing protocols (OSPF, EIGRP). No extra VPN protocols required.


#                    STP (Spanning-tree protocol)
https://linkmeup.ru/podcasts/977/
STP (802.1D) - protocol that allows switches to become aware of other switches through advertising and receiving of BPDUs. Also build loop-free topology
BPDU - Bridge Protocol Data Unit

STP port states:
Disabled - Port is admin down
Blocking - Port cannot forward network traffic. Can receive BPDUs
Listening - Can't forward network traffic. Can receive and forward BPDUs
Learning - Can't forward network traffic. Can receive and send BPDUs and alter MAC address table based on received network traffic 
Forwarding - Port operates as usual
Broken - Port discards packets while issue exists
